{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading all necessary libraries and setting up the GPU config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(allow_growth = True)\n",
    "config=tf.ConfigProto(gpu_options=gpu_options)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the number of files in the training set. The max_length parameter below is used to capture the maximum length of time domain across all the train, test and validation signals. I use this parameter to pad all my signals with zeros up to the max_length. Sampling rate is set to 16000 which is common to all signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No. of files\n",
    "\n",
    "n_file = 1200\n",
    "\n",
    "max_length = 200\n",
    "\n",
    "sr = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will load files from a particular directory (training, test or validation). It then computes STFT for all those loaded signals, gets the absolute values, and the lengths of all the signals. Each of these parameters are also appended to separate lists and returned. Note that I also perform padding of the abs(STFT(s)) here. For example, if the STFT of a signal results in a matrix sized (513 X 90), then I pad 110 columns after the 90th column with zeroes to make the number of columns equal to 200. It turned out that I did not require to do so in the end, but you will see I have made sure that this padding does not affect the functioning and the correctness of the code, as per my knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the file, performing STFT, taking absolute, padding zeros as required\n",
    "\n",
    "def loadfile(path, str_tr, flag = 0):\n",
    "    list_tr = []\n",
    "    list_stft = []\n",
    "    list_stft_abs = []\n",
    "    list_length = []\n",
    "    z = ['000', '00', '0', '']\n",
    "    \n",
    "    for i in range(n_file):\n",
    "        if (i == 0):\n",
    "            j = 0\n",
    "        else:\n",
    "            j = int(math.log10(i))\n",
    "        s, sr = librosa.load(path + str_tr + z[j] + str(i) + '.wav', sr = None)\n",
    "        if (flag == 1):\n",
    "            list_tr.append(s)\n",
    "        \n",
    "        #Calculating STFT\n",
    "        stft = librosa.stft(s, n_fft= 1024, hop_length= 512)\n",
    "        \n",
    "        stft_len = stft.shape[1]\n",
    "        \n",
    "        #Appending STFT to list\n",
    "        if (flag == 1):\n",
    "            list_stft.append(stft)\n",
    "        \n",
    "        #Calculating Absolute of STFT\n",
    "        stft_abs = np.abs(stft)\n",
    "        \n",
    "        #Padding zeros to make length 300\n",
    "        stft_abs = np.pad(stft_abs, ((0,0),(0, max_length-stft_len)), 'constant')\n",
    "        \n",
    "        #Appending abs to list\n",
    "        list_stft_abs.append(stft_abs)\n",
    "        \n",
    "        #Appending time-length of STFT to list\n",
    "        list_length.append(stft_len)\n",
    "        \n",
    "    return list_tr, list_stft, list_stft_abs, list_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the path to load the training signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path of the training signals\n",
    "path = \"/opt/e533/timit-homework/tr/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading all the training signals below. trx, trs and trn are lists that correspond to the speech signals in time domain. X, S, and N correspond to the STFT of the signals. X_abs, S_abs, N_abs are lists that correspond to the absolute valued STFTs. X_len, S_len, and N_len correspond to the lists storing lengths of each of the loaded signals. This notation remains pretty consistent throughout the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading all training noisy speech signals\n",
    "\n",
    "trx, X, X_abs, X_len = loadfile(path, 'trx')\n",
    "\n",
    "#Loading all training clean speech signals\n",
    "\n",
    "trs, S, S_abs, S_len = loadfile(path, 'trs')\n",
    "\n",
    "#Loading all training noise signals\n",
    "\n",
    "trn, N, N_abs, N_len = loadfile(path, 'trn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below computes the IBM, given the absolute valued STFTs of the clean speech and the corresponding noise signals, and returns a list of the IBMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IBM(S, N):\n",
    "    M = []\n",
    "    \n",
    "    for i in range(len(S)):\n",
    "        m_ibm = 1 * (S[i] > N[i])\n",
    "        M.append(m_ibm)\n",
    "    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching IBMs for the training Speech and Noise signals. These serve as the ground truth for the training of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Binary Masks from S_abs and N_abs\n",
    "M = IBM(S_abs, N_abs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the batch size and keep_pr is a placeholder for the keep probability of the dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "#Keep probability for dropouts\n",
    "keep_pr = tf.placeholder(tf.float32, ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining frame size (frame_size) which is the length of the 3rd dimension of the input. num_hidden is the number of hidden layers in the LSTM cell. seq_len takes in the length of time domain for all the signals that would be passed in one batch. This was done to support the padding we had done earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Placeholders for input to the network\n",
    "\n",
    "frame_size = 513\n",
    "num_hidden = 256\n",
    "seq_len = tf.placeholder(tf.int32, None)\n",
    "\n",
    "q2_x = tf.placeholder(tf.float32, [None, max_length, frame_size])\n",
    "q2_y = tf.placeholder(tf.float32, [None, max_length, frame_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the RNN below. I have used one RNN with LSTM Cell with 128 hidden layers, whose weights are initialized with Xavier. Then I add a dense layer to the output so that I can get the dimensions of the network output equal to the ground truth output. I use sigmoid activation in the end to contrain output between 0 to 1. \n",
    "\n",
    "Note the 'dim' variable. It stores the length of time domain of the inputs in the training batch. Just storing the time domain length of the first input because the way I pass inputs in a batch, all of them have the same length in the time domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the RNN\n",
    "output, state = tf.nn.dynamic_rnn(tf.nn.rnn_cell.DropoutWrapper(tf.contrib.rnn.LSTMCell(num_hidden, \n",
    "                                                         initializer = tf.contrib.layers.xavier_initializer()),\n",
    "                                                                output_keep_prob = keep_pr), \n",
    "                                  q2_x, dtype=tf.float32, sequence_length=seq_len)\n",
    "\n",
    "rnn_out = tf.layers.dense(output, 513, kernel_initializer= tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "dim = seq_len[0]\n",
    "\n",
    "fin_out = tf.sigmoid(rnn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining learning rate below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using MSE as the loss function. Note that I exclude the padding while calculating the MSE. This takes care that I do not underestimate the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.losses.mean_squared_error(fin_out[:, :dim,:], q2_y[:, :dim, :]))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate= lr).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intitializing the session and a saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init all TF vars and run the session\n",
    "\n",
    "sess = tf.Session(config = config)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "error = np.zeros(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network below. Note that the batches are consistent in each epoch (have the same set of signals), but I do change the ordering of the batches passed in each epoch. The swapaxes code swaps the axis on the signals in a manner that can be passed to the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed out of  100 ; loss:  25.92656147480011\n",
      "Epoch 2 completed out of  100 ; loss:  22.47987288236618\n",
      "Epoch 3 completed out of  100 ; loss:  20.69874981045723\n",
      "Epoch 4 completed out of  100 ; loss:  19.52434667944908\n",
      "Epoch 5 completed out of  100 ; loss:  18.64920210838318\n",
      "Epoch 6 completed out of  100 ; loss:  17.93918462842703\n",
      "Epoch 7 completed out of  100 ; loss:  17.531486600637436\n",
      "Epoch 8 completed out of  100 ; loss:  17.18608297407627\n",
      "Epoch 9 completed out of  100 ; loss:  16.84702058136463\n",
      "Epoch 10 completed out of  100 ; loss:  16.597764022648335\n",
      "Epoch 11 completed out of  100 ; loss:  16.41173230111599\n",
      "Epoch 12 completed out of  100 ; loss:  16.02383080124855\n",
      "Epoch 13 completed out of  100 ; loss:  15.841072976589203\n",
      "Epoch 14 completed out of  100 ; loss:  15.670962944626808\n",
      "Epoch 15 completed out of  100 ; loss:  15.514885418117046\n",
      "Epoch 16 completed out of  100 ; loss:  15.291068434715271\n",
      "Epoch 17 completed out of  100 ; loss:  15.30916078388691\n",
      "Epoch 18 completed out of  100 ; loss:  15.103117488324642\n",
      "Epoch 19 completed out of  100 ; loss:  14.812072768807411\n",
      "Epoch 20 completed out of  100 ; loss:  14.578663021326065\n",
      "Epoch 21 completed out of  100 ; loss:  14.49416945874691\n",
      "Epoch 22 completed out of  100 ; loss:  14.283257715404034\n",
      "Epoch 23 completed out of  100 ; loss:  14.154000528156757\n",
      "Epoch 24 completed out of  100 ; loss:  14.004856429994106\n",
      "Epoch 25 completed out of  100 ; loss:  13.898460619151592\n",
      "Epoch 26 completed out of  100 ; loss:  13.77019303292036\n",
      "Epoch 27 completed out of  100 ; loss:  13.702720426023006\n",
      "Epoch 28 completed out of  100 ; loss:  13.585483841598034\n",
      "Epoch 29 completed out of  100 ; loss:  13.478754848241806\n",
      "Epoch 30 completed out of  100 ; loss:  13.46878669410944\n",
      "Epoch 31 completed out of  100 ; loss:  13.282740250229836\n",
      "Epoch 32 completed out of  100 ; loss:  13.237219110131264\n",
      "Epoch 33 completed out of  100 ; loss:  14.195843003690243\n",
      "Epoch 34 completed out of  100 ; loss:  13.370230160653591\n",
      "Epoch 35 completed out of  100 ; loss:  13.041438639163971\n",
      "Epoch 36 completed out of  100 ; loss:  12.89930558949709\n",
      "Epoch 37 completed out of  100 ; loss:  12.789537787437439\n",
      "Epoch 38 completed out of  100 ; loss:  12.710137411952019\n",
      "Epoch 39 completed out of  100 ; loss:  12.718276388943195\n",
      "Epoch 40 completed out of  100 ; loss:  12.632175639271736\n",
      "Epoch 41 completed out of  100 ; loss:  12.554555661976337\n",
      "Epoch 42 completed out of  100 ; loss:  12.496427901089191\n",
      "Epoch 43 completed out of  100 ; loss:  12.423515774309635\n",
      "Epoch 44 completed out of  100 ; loss:  12.537788815796375\n",
      "Epoch 45 completed out of  100 ; loss:  12.512284889817238\n",
      "Epoch 46 completed out of  100 ; loss:  12.34867937117815\n",
      "Epoch 47 completed out of  100 ; loss:  12.268680453300476\n",
      "Epoch 48 completed out of  100 ; loss:  12.205295220017433\n",
      "Epoch 49 completed out of  100 ; loss:  12.144578754901886\n",
      "Epoch 50 completed out of  100 ; loss:  12.097686469554901\n",
      "Epoch 51 completed out of  100 ; loss:  12.073864214122295\n",
      "Epoch 52 completed out of  100 ; loss:  12.068847641348839\n",
      "Epoch 53 completed out of  100 ; loss:  12.128489524126053\n",
      "Epoch 54 completed out of  100 ; loss:  12.167677663266659\n",
      "Epoch 55 completed out of  100 ; loss:  12.068101979792118\n",
      "Epoch 56 completed out of  100 ; loss:  11.942215032875538\n",
      "Epoch 57 completed out of  100 ; loss:  11.842683523893356\n",
      "Epoch 58 completed out of  100 ; loss:  11.799282528460026\n",
      "Epoch 59 completed out of  100 ; loss:  11.783257082104683\n",
      "Epoch 60 completed out of  100 ; loss:  11.804201528429985\n",
      "Epoch 61 completed out of  100 ; loss:  11.857037626206875\n",
      "Epoch 62 completed out of  100 ; loss:  11.750119119882584\n",
      "Epoch 63 completed out of  100 ; loss:  11.69269385933876\n",
      "Epoch 64 completed out of  100 ; loss:  11.644701398909092\n",
      "Epoch 65 completed out of  100 ; loss:  12.501556649804115\n",
      "Epoch 66 completed out of  100 ; loss:  11.833585396409035\n",
      "Epoch 67 completed out of  100 ; loss:  11.712047956883907\n",
      "Epoch 68 completed out of  100 ; loss:  11.579634606838226\n",
      "Epoch 69 completed out of  100 ; loss:  11.534781709313393\n",
      "Epoch 70 completed out of  100 ; loss:  11.495386093854904\n",
      "Epoch 71 completed out of  100 ; loss:  11.480478249490261\n",
      "Epoch 72 completed out of  100 ; loss:  11.445359572768211\n",
      "Epoch 73 completed out of  100 ; loss:  11.422653935849667\n",
      "Epoch 74 completed out of  100 ; loss:  11.399315230548382\n",
      "Epoch 75 completed out of  100 ; loss:  11.381817765533924\n",
      "Epoch 76 completed out of  100 ; loss:  11.353985279798508\n",
      "Epoch 77 completed out of  100 ; loss:  11.380787335336208\n",
      "Epoch 78 completed out of  100 ; loss:  11.36110644042492\n",
      "Epoch 79 completed out of  100 ; loss:  11.334053605794907\n",
      "Epoch 80 completed out of  100 ; loss:  11.315451703965664\n",
      "Epoch 81 completed out of  100 ; loss:  11.297116197645664\n",
      "Epoch 82 completed out of  100 ; loss:  11.266781963407993\n",
      "Epoch 83 completed out of  100 ; loss:  11.306061454117298\n",
      "Epoch 84 completed out of  100 ; loss:  11.264810122549534\n",
      "Epoch 85 completed out of  100 ; loss:  11.279907695949078\n",
      "Epoch 86 completed out of  100 ; loss:  11.196937188506126\n",
      "Epoch 87 completed out of  100 ; loss:  11.173082366585732\n",
      "Epoch 88 completed out of  100 ; loss:  11.140479408204556\n",
      "Epoch 89 completed out of  100 ; loss:  11.119139693677425\n",
      "Epoch 90 completed out of  100 ; loss:  11.1158497184515\n",
      "Epoch 91 completed out of  100 ; loss:  11.162667408585548\n",
      "Epoch 92 completed out of  100 ; loss:  11.191047728061676\n",
      "Epoch 93 completed out of  100 ; loss:  11.322035476565361\n",
      "Epoch 94 completed out of  100 ; loss:  11.24701938033104\n",
      "Epoch 95 completed out of  100 ; loss:  11.096104763448238\n",
      "Epoch 96 completed out of  100 ; loss:  11.020805284380913\n",
      "Epoch 97 completed out of  100 ; loss:  10.985534779727459\n",
      "Epoch 98 completed out of  100 ; loss:  10.975818052887917\n",
      "Epoch 99 completed out of  100 ; loss:  10.983837723731995\n",
      "Epoch 100 completed out of  100 ; loss:  10.976574495434761\n"
     ]
    }
   ],
   "source": [
    "#Training the network with shuffling of training batches\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    random = np.arange(0, 1200, 10)\n",
    "    np.random.shuffle(random)\n",
    "    for i in range(len(random)):\n",
    "        start = int(random[i])\n",
    "        end = int(start + batch_size)\n",
    "        epoch_y = np.array(M[start:end]).swapaxes(1,2)\n",
    "        epoch_x = np.array(X_abs[start:end]).swapaxes(1,2)\n",
    "        seqlen = np.array(X_len[start:end])\n",
    "        l, _ = sess.run([cost, optimizer], feed_dict = {q2_x: epoch_x, q2_y: epoch_y, seq_len: seqlen, keep_pr: 1})\n",
    "        error[epoch] += l\n",
    "    \n",
    "    print('Epoch', epoch+1, 'completed out of ', epochs,'; loss: ', error[epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q2model/q2'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, 'q2model/q2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the validation set below in the same way as done for the training set previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading validation set\n",
    "\n",
    "path = \"/opt/e533/timit-homework/v/\"\n",
    "\n",
    "vx, VX, VX_abs, VX_len = loadfile(path, 'vx', flag = 1)\n",
    "\n",
    "vs, VS, VS_abs, VS_len = loadfile(path, 'vs', flag = 1)\n",
    "\n",
    "vn, VN, VN_abs, VN_len = loadfile(path, 'vn', flag = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching the binary masks for the clean speech and noise validation signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Binary Masks from S_abs and N_abs\n",
    "VM = IBM(VS_abs, VN_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init variable to store SNR of validation sets\n",
    "\n",
    "SNR_Val = np.zeros(1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function below calculates the SNR for each of the predicted signals in the validation set. Returns the list of SNRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_SNR(M_pred, X, s, i):\n",
    "    M_pred = 1 * (M_pred > 0.5)\n",
    "    M_pred = M_pred.T\n",
    "    S_pred = M_pred * X\n",
    "    s_pred = librosa.istft(S_pred, win_length = 1024, hop_length = 512)\n",
    "    \n",
    "    nlen = min(len(s), len(s_pred))\n",
    "    SNR = 10*math.log10((np.sum(s[:nlen]**2))/(np.sum((s[:nlen] - s_pred[:nlen])**2)))\n",
    "    \n",
    "    return SNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting prediction for all validation sets from the trained network. Note that when I call the calc_SNR function, I remove the padded columns from the predicted masks. So, this is where you can see that I ensure padding does not get into the way of the authenticity of the results, again, as per my knowledge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions for all validation sets\n",
    "for i in range(len(VX_abs)):\n",
    "    epoch_x = np.zeros((1, VX_abs[i].shape[1], VX_abs[i].shape[0]))\n",
    "    epoch_y = np.zeros((1, VX_abs[i].shape[1], VX_abs[i].shape[0]))\n",
    "    \n",
    "    epoch_x[0,:,:] = VX_abs[i].T\n",
    "    epoch_y[0,:,:] = VS_abs[i].T\n",
    "    \n",
    "    VM_pred, val_loss = sess.run([fin_out, cost], feed_dict = {q2_x:epoch_x, q2_y:epoch_y, seq_len : np.array([VX_len[i]]), keep_pr:1})\n",
    "    \n",
    "    SNR_Val[i] = calc_SNR(VM_pred[0,:VX_len[i],:], VX[i], vs[i], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the mean value of the SNR below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.637977559016333\n"
     ]
    }
   ],
   "source": [
    "#Calculating mean SNR\n",
    "\n",
    "print(np.mean(SNR_Val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the validation set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading test set\n",
    "\n",
    "path = \"/opt/e533/timit-homework/te/\"\n",
    "\n",
    "n_file = 400\n",
    "\n",
    "tex, TEX, TEX_abs, TEX_len = loadfile(path, 'tex', flag = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function below writes out predicted test signals. 'tex0000.wav' is written out as 'Test_Recover_0.wav', 'tex1923.wav' as 'Test_Recover_1923.wav' and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_SNR(M_pred, X, i):\n",
    "    M_pred = 1 * (M_pred > 0.5)\n",
    "    M_pred = M_pred.T\n",
    "    S_pred = M_pred * X\n",
    "    s_pred = librosa.istft(S_pred, win_length = 1024, hop_length = 512)\n",
    "    \n",
    "    librosa.output.write_wav('TestC/Test_Recover_' + str(i) + '.wav', s_pred, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting prediction for the test set and storing them. I have attached all those signals in the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions for all test sets\n",
    "for i in range(len(TEX_abs)):\n",
    "    epoch_x = np.zeros((1, TEX_abs[i].shape[1], TEX_abs[i].shape[0]))\n",
    "    epoch_y = np.zeros((1, TEX_abs[i].shape[1], TEX_abs[i].shape[0]))\n",
    "    \n",
    "    epoch_x[0,:,:] = TEX_abs[i].T\n",
    "    #epoch_y[0,:,:] = TEabs[i].T\n",
    "    \n",
    "    TEM_pred= sess.run(fin_out, feed_dict = {q2_x:epoch_x, seq_len : np.array([TEX_len[i]]), keep_pr: 1})\n",
    "    \n",
    "    test_SNR(TEM_pred[0,:TEX_len[i],:], TEX[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
